"""
åƒåœ¾é‚®ä»¶æ•°æ®å¯¼å…¥API
è·¯å¾„: src/api/data/import_spam_api.py
æ¥å£: POST /api/data/import_spam
"""

import re
import json
import uuid
import pymysql
import pandas as pd
from dotenv import load_dotenv
from email import message_from_string
from email.utils import parsedate_to_datetime
import os
import sys
from typing import List, Dict, Tuple
from flask import Blueprint, request, jsonify
from werkzeug.utils import secure_filename
import tempfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_file = os.path.abspath(__file__)
data_dir = os.path.dirname(current_file)  # src/api/data
api_dir = os.path.dirname(data_dir)  # src/api
src_dir = os.path.dirname(api_dir)  # src
project_root = os.path.dirname(src_dir)  # é¡¹ç›®æ ¹ç›®å½•

if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.logger import logger
from data.db_init import get_db_connection

# ===================== åŸºç¡€é…ç½® =====================
load_dotenv()

BATCH_SIZE = 100
MAX_CONTENT_LENGTH = 65535
ALLOWED_EXTENSIONS = {'csv'}

# åˆ›å»ºè“å›¾
import_spam_bp = Blueprint('import_spam', __name__)

# ===================== æ ¸å¿ƒå‡½æ•° =====================
def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶ç±»å‹"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def read_spam_csv(file_path: str) -> List[Tuple[str, str]]:
    """è¯»å–CSVæ–‡ä»¶"""
    try:
        df = pd.read_csv(
            file_path,
            encoding="latin-1",
            quotechar='"',
            escapechar="\\",
            on_bad_lines="skip",
            dtype={"target": str}
        )
        
        if "text" not in df.columns or "target" not in df.columns:
            raise ValueError("CSVå¿…é¡»åŒ…å« 'text' å’Œ 'target' åˆ—")
        
        df = df.dropna(subset=["text", "target"])
        df["text"] = df["text"].astype(str).str.strip()
        df["target"] = df["target"].astype(str).str.strip()
        df = df[df["text"].str.len() > 50]
        df = df[df["target"].isin(["0", "1"])]
        
        data = []
        for _, row in df.iterrows():
            email_content = row["text"]
            # ä¿®å¤1ï¼šlabelå€¼ä»å­—ç¬¦ä¸²"phishing/normal"æ”¹ä¸ºæ•°å­—1/0ï¼ˆåŒ¹é…email_dataè¡¨çš„labelå­—æ®µï¼‰
            label = 1 if row["target"] == "1" else 0
            data.append((email_content, label))
        
        logger.info(f"âœ… æˆåŠŸè¯»å–CSVï¼Œæœ‰æ•ˆæ•°æ®ï¼š{len(data)}æ¡")
        return data
    
    except Exception as e:
        logger.error(f"âŒ è¯»å–CSVå¤±è´¥ï¼š{str(e)}", exc_info=True)
        raise


def preprocess_mbox_format(raw_content: str) -> str:
    """é¢„å¤„ç†mboxæ ¼å¼"""
    header_pattern = r'(\b(?:Return-Path|Delivered-To|Received|Date|From|To|Subject|Message-Id|MIME-Version|Content-Type|Content-Transfer-Encoding|X-[\w-]+|In-Reply-To|References|Sender|Errors-To|List-[\w-]+|Reply-To|Cc|Bcc|User-Agent|X-Mailer|Importance|Priority):\s*)'
    formatted = re.sub(header_pattern, r'\n\1', raw_content, flags=re.IGNORECASE)
    formatted = re.sub(r'\n{2,}', '\n', formatted).strip()
    return formatted


def extract_urls(text: str) -> List[str]:
    """æå–æ–‡æœ¬ä¸­çš„æ‰€æœ‰URL"""
    if not text:
        return []
    url_pattern = r'https?://[^\s<>"\'\)]+|www\.[^\s<>"\'\)]+'
    urls = re.findall(url_pattern, text, re.IGNORECASE)
    return list(set([url[:500] for url in urls if len(url) < 500]))


def parse_email_optimized(raw_content: str, label: str) -> Dict:
    """æœ€ä¼˜é‚®ä»¶è§£ææ–¹æ¡ˆ"""
    try:
        formatted_content = preprocess_mbox_format(raw_content)
        msg = message_from_string(formatted_content)
        
        sender = "unknown"
        from_header = msg.get("From", "")
        if from_header:
            email_match = re.search(r'[\w\.\-]+@[\w\.\-]+\.\w+', from_header)
            if email_match:
                sender = email_match.group(0)
        
        recipient = "unknown"
        to_header = msg.get("To", "")
        if to_header:
            email_match = re.search(r'[\w\.\-]+@[\w\.\-]+\.\w+', to_header)
            if email_match:
                recipient = email_match.group(0)
        
        subject = msg.get("Subject", "no subject").strip()
        if not subject:
            subject = "no subject"
        
        send_time = None
        date_header = msg.get("Date", "")
        if date_header:
            try:
                send_time = parsedate_to_datetime(date_header)
            except:
                pass
        
        content_text = ""
        
        if msg.is_multipart():
            for part in msg.walk():
                content_type = part.get_content_type()
                if content_type == "text/plain":
                    try:
                        payload = part.get_payload(decode=True)
                        if payload:
                            content_text = payload.decode('utf-8', errors='ignore')
                            break
                    except Exception as e:
                        logger.debug(f"è§£ç multipartå¤±è´¥ï¼š{e}")
                        continue
        else:
            try:
                payload = msg.get_payload(decode=True)
                if payload:
                    content_text = payload.decode('utf-8', errors='ignore')
                else:
                    payload_str = msg.get_payload()
                    if isinstance(payload_str, str):
                        content_text = payload_str
            except Exception as e:
                logger.debug(f"è§£ç å•éƒ¨åˆ†å¤±è´¥ï¼š{e}")
                try:
                    content_text = str(msg.get_payload())
                except:
                    pass
        
        if not content_text or len(content_text.strip()) < 20:
            body_match = re.search(r'\n\s*\n(.+)', formatted_content, re.DOTALL)
            if body_match:
                content_text = body_match.group(1).strip()
            else:
                content_text = raw_content
        
        lines = content_text.split('\n')
        clean_lines = []
        for line in lines:
            if re.match(r'^[A-Za-z\-]+:\s*', line):
                continue
            clean_lines.append(line)
        
        content_text = '\n'.join(clean_lines)
        content_text = re.sub(r'[\x00-\x1F\x7F]', '', content_text)
        content_text = re.sub(r'\n{3,}', '\n\n', content_text)
        content_text = content_text.strip()
        
        if len(content_text) > MAX_CONTENT_LENGTH:
            content_text = content_text[:MAX_CONTENT_LENGTH]
        
        urls = extract_urls(content_text)
        # ä¿®å¤2ï¼šå­—æ®µåä»url_links_jsonæ”¹ä¸ºurl_list_jsonï¼ˆåŒ¹é…email_dataè¡¨çš„url_listå­—æ®µï¼‰
        url_list_json = json.dumps(urls, ensure_ascii=False) if urls else None
        
        return {
            # ä¿®å¤3ï¼škeyåä»email_uuidæ”¹ä¸ºemail_idï¼ˆåŒ¹é…email_dataè¡¨çš„email_idå­—æ®µï¼‰
            "email_id": str(uuid.uuid4()),
            "sender": sender[:255],
            "recipient": recipient[:255],
            "subject": subject[:500],
            "send_time": send_time,
            "label": label,
            "content_text": content_text or "no content",
            # ä¿®å¤4ï¼šå­—æ®µåä»url_linksæ”¹ä¸ºurl_listï¼ˆåŒ¹é…email_dataè¡¨çš„url_listå­—æ®µï¼‰
            "url_list": url_list_json
        }
    
    except Exception as e:
        logger.warning(f"âš ï¸ é‚®ä»¶è§£æå¤±è´¥ï¼š{str(e)}")
        return {
            # ä¿®å¤3ï¼škeyåä»email_uuidæ”¹ä¸ºemail_id
            "email_id": str(uuid.uuid4()),
            "sender": "parse_failed",
            "recipient": "unknown",
            "subject": "parse error",
            "send_time": None,
            "label": label,
            "content_text": raw_content[:MAX_CONTENT_LENGTH],
            # ä¿®å¤4ï¼šå­—æ®µåä»url_linksæ”¹ä¸ºurl_list
            "url_list": None
        }


def batch_write_to_mysql(parsed_data: List[Dict]):
    """æ‰¹é‡å†™å…¥æ•°æ®åº“"""
    if not parsed_data:
        logger.warning("âš ï¸ æ— æœ‰æ•ˆæ•°æ®å¯å†™å…¥")
        return 0
    
    valid_data = [d for d in parsed_data if d is not None]
    if not valid_data:
        logger.warning("âš ï¸ è¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®")
        return 0
    
    conn = None
    try:
        conn = get_db_connection(use_db=True)
        cursor = conn.cursor()
        
        # ä¿®å¤5ï¼šåˆ é™¤SQLä¸­ä¸å­˜åœ¨çš„has_attachment/attachment_countå­—æ®µ
        insert_sql = """
        INSERT IGNORE INTO email_data (
            email_id, sender, recipient, subject, send_time, 
            label, content_text, url_list, data_version
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        total = len(valid_data)
        success_count = 0
        
        for i in range(0, total, BATCH_SIZE):
            batch = valid_data[i:i+BATCH_SIZE]
            batch_data = [
                (
                    # ä¿®å¤6ï¼šå–å€¼ä»d["email_uuid"]æ”¹ä¸ºd["email_id"]
                    d["email_id"],
                    d["sender"],
                    d["recipient"],
                    d["subject"],
                    d["send_time"],
                    d["label"],
                    d["content_text"],
                    # ä¿®å¤7ï¼šå–å€¼ä»d["url_links"]æ”¹ä¸ºd["url_list"]
                    d["url_list"],
                    1  # data_versionå›ºå®šå€¼
                ) for d in batch
            ]
            
            try:
                cursor.executemany(insert_sql, batch_data)
                conn.commit()
                success_count += len(batch)
                logger.info(f"ğŸ“¤ æ‰¹æ¬¡ {i//BATCH_SIZE + 1}ï¼šå·²æ’å…¥ {len(batch)} æ¡ï¼Œç´¯è®¡ {success_count}/{total}")
            except pymysql.MySQLError as e:
                conn.rollback()
                logger.error(f"âŒ æ‰¹æ¬¡ {i//BATCH_SIZE + 1} å¤±è´¥ï¼š{e.args[1]}")
                continue
        
        logger.info(f"ğŸ‰ å…¥åº“å®Œæˆï¼šæˆåŠŸ {success_count}/{total} æ¡")
        return success_count
    
    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥ï¼š{str(e)}", exc_info=True)
        raise
    finally:
        if conn:
            cursor.close()
            conn.close()


# ===================== APIè·¯ç”± =====================

@import_spam_bp.route('/import_spam', methods=['POST'])
def import_spam():
    """
    å¯¼å…¥åƒåœ¾é‚®ä»¶æ•°æ®API
    è¯·æ±‚: multipart/form-data
    å‚æ•°: file (CSVæ–‡ä»¶)
    """
    try:
        # é»˜è®¤ä½¿ç”¨æœåŠ¡å™¨ä¸Šçš„æ–‡ä»¶
        file_path = os.path.join(project_root, "data", "spam_assassin.csv")
        
        # è¯»å–CSV
        raw_data = read_spam_csv(file_path)
        
        # è§£æé‚®ä»¶
        logger.info("å¼€å§‹è§£æé‚®ä»¶...")
        parsed_data = []
        for idx, (content, label) in enumerate(raw_data):
            if idx % 500 == 0:
                logger.info(f"ğŸ” å·²è§£æ {idx}/{len(raw_data)} å°")
            parsed_email = parse_email_optimized(content, label)
            parsed_data.append(parsed_email)
        
        # å†™å…¥æ•°æ®åº“
        success_count = batch_write_to_mysql(parsed_data)
        
        logger.info("âœ… å…¨éƒ¨æµç¨‹å®Œæˆï¼")
        
        return jsonify({
            'success': True,
            'total': len(raw_data),
            'success_count': success_count,
            'message': f'å¯¼å…¥å®Œæˆï¼æˆåŠŸ {success_count}/{len(raw_data)} æ¡'
        }), 200
    
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥ï¼š{str(e)}", exc_info=True)
        return jsonify({
            'success': False,
            'message': f'å¯¼å…¥å¤±è´¥: {str(e)}'
        }), 500