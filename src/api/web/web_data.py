import os
import sys
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify

# é¡¹ç›®æ ¹è·¯å¾„é…ç½®
current_file = os.path.abspath(__file__)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥å·¥å…·æ¨¡å—
from src.utils.logger import logger
from data.db_init import get_db_connection

# åˆ›å»ºè“å›¾
web_dashboard_bp = Blueprint('web_dashboard', __name__)


class DatabaseHelper:
    """æ•°æ®åº“è¾…åŠ©ç±»ï¼Œå…¼å®¹æ‰€æœ‰æ•°æ®åº“é©±åŠ¨"""
    
    @staticmethod
    def fetchone_as_dict(cursor):
        """å°†å•æ¡è®°å½•è½¬ä¸ºå­—å…¸"""
        row = cursor.fetchone()
        if not row:
            return None
        
        columns = [col[0] for col in cursor.description]
        
        # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥è¿”å›
        if isinstance(row, dict):
            return row
        
        # å¦‚æœæ˜¯å…ƒç»„æˆ–åˆ—è¡¨ï¼Œè½¬ä¸ºå­—å…¸
        if isinstance(row, (tuple, list)):
            return dict(zip(columns, row))
        
        return None
    
    @staticmethod
    def fetchall_as_dict(cursor):
        """å°†æ‰€æœ‰è®°å½•è½¬ä¸ºå­—å…¸åˆ—è¡¨"""
        rows = cursor.fetchall()
        if not rows:
            return []
        
        columns = [col[0] for col in cursor.description]
        
        result = []
        for row in rows:
            # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥æ·»åŠ 
            if isinstance(row, dict):
                result.append(row)
            # å¦‚æœæ˜¯å…ƒç»„æˆ–åˆ—è¡¨ï¼Œè½¬ä¸ºå­—å…¸
            elif isinstance(row, (tuple, list)):
                result.append(dict(zip(columns, row)))
        
        return result


def parse_time_range(time_range_str):
    """è§£ææ—¶é—´èŒƒå›´å‚æ•°"""
    end_time = datetime.now()
    
    if time_range_str.endswith('h'):
        hours = int(time_range_str[:-1])
        start_time = end_time - timedelta(hours=hours)
    elif time_range_str.endswith('d'):
        days = int(time_range_str[:-1])
        start_time = end_time - timedelta(days=days)
    else:
        try:
            start_ts, end_ts = time_range_str.split('-')
            start_time = datetime.fromtimestamp(int(start_ts) / 1000)
            end_time = datetime.fromtimestamp(int(end_ts) / 1000)
        except:
            start_time = end_time - timedelta(hours=12)
    
    return start_time, end_time


def get_time_bucket_size(start_time, end_time):
    """æ ¹æ®æ—¶é—´èŒƒå›´è‡ªåŠ¨ç¡®å®šèšåˆç²’åº¦"""
    duration = (end_time - start_time).total_seconds()
    
    if duration <= 3600 * 12:
        return 600, '%H:%M'
    elif duration <= 3600 * 24:
        return 1800, '%H:%M'
    elif duration <= 3600 * 24 * 3:
        return 3600, '%m-%d %H:00'
    elif duration <= 3600 * 24 * 7:
        return 7200, '%m-%d %H:00'
    else:
        return 86400, '%Y-%m-%d'


def _fill_missing_time_buckets(raw_data, start_time, end_time, bucket_size, time_format, trend_keys):
    """
    å¡«å……ç¼ºå¤±çš„æ—¶é—´æ¡¶æ•°æ®ï¼Œç¡®ä¿æ—¶é—´è½´å®Œæ•´ã€‚
    
    :param raw_data: æ•°æ®åº“æŸ¥è¯¢è¿”å›çš„åŸå§‹æ•°æ®åˆ—è¡¨ã€‚
    :param start_time: æŸ¥è¯¢çš„èµ·å§‹æ—¶é—´ã€‚
    :param end_time: æŸ¥è¯¢çš„ç»“æŸæ—¶é—´ã€‚
    :param bucket_size: æ—¶é—´æ¡¶å¤§å°ï¼ˆç§’ï¼‰ã€‚
    :param time_format: æ—¶é—´æ ¼å¼åŒ–å­—ç¬¦ä¸²ã€‚
    :param trend_keys: éœ€è¦å¡«å……çš„è¶‹åŠ¿æ•°æ®é”®ååˆ—è¡¨ã€‚
    :return: å¡«å……åçš„è¶‹åŠ¿æ•°æ®å­—å…¸ã€‚
    """
    
    # 1. åˆå§‹åŒ–ç©ºè¶‹åŠ¿æ•°æ®
    filled_data = {'timestamps': []}
    for key in trend_keys:
        filled_data[key] = []

    # 2. å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºä»¥ time_bucket_id ä¸ºé”®çš„å­—å…¸
    trend_map = {row['time_bucket']: row for row in raw_data}

    # 3. è®¡ç®—ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ—¶é—´æ¡¶çš„ç§’æ•°ï¼Œå¹¶ç¡®ä¿åŒ…å«æœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ—¶é—´æ¡¶
    start_ts_sec = int(start_time.timestamp()) // bucket_size * bucket_size
    
    # æœ€åä¸€ä¸ªæ—¶é—´æ¡¶çš„ç§’æ•°ï¼Œç¡®ä¿è‡³å°‘åŒ…å«end_timeæ‰€åœ¨çš„æ—¶é—´æ¡¶
    end_ts_sec = int(end_time.timestamp()) // bucket_size * bucket_size
    if end_time.timestamp() % bucket_size != 0:
        end_ts_sec += bucket_size
    
    # 4. éå†å®Œæ•´çš„æ—¶é—´è½´å¹¶å¡«å……æ•°æ®
    current_ts_sec = start_ts_sec
    while current_ts_sec <= end_ts_sec:
        time_bucket_id = current_ts_sec // bucket_size
        
        # è½¬æ¢ä¸º datetime å¯¹è±¡è¿›è¡Œæ ¼å¼åŒ–
        timestamp = datetime.fromtimestamp(current_ts_sec)
        
        # æŸ¥æ‰¾æ•°æ®æˆ–ä½¿ç”¨ 0 å¡«å……
        row = trend_map.get(time_bucket_id, {})
        
        filled_data['timestamps'].append(timestamp.strftime(time_format))
        
        for key in trend_keys:
            # ä½¿ç”¨ get(key, 0) ç¡®ä¿ç¼ºå¤±æ—¶ä¸º 0
            filled_data[key].append(int(row.get(key, 0)))
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ¡¶
        current_ts_sec += bucket_size
        
    return filled_data


@web_dashboard_bp.route('/dashboard', methods=['GET'])
def get_dashboard_data():
    """è·å–ä»ªè¡¨ç›˜æ•°æ®"""
    conn = None
    cursor = None
    
    try:
        time_range = request.args.get('timeRange', '12h')
        # å¼ºåˆ¶ä½¿ç”¨å½“å‰æ—¶é—´æ¥è§£ææ—¶é—´èŒƒå›´
        start_time, end_time = parse_time_range(time_range) 
        bucket_size, time_format = get_time_bucket_size(start_time, end_time)
        
        logger.info(f"Dashboard query: {time_range}, from {start_time} to {end_time}")
        
        conn = get_db_connection()
        cursor = conn.cursor()
        db_helper = DatabaseHelper()
        
        # ============ 1. è·å–æ±‡æ€»æ•°æ® ============
        summary_query = """
        SELECT 
            COUNT(*) as total_count,
            COALESCE(SUM(CASE WHEN final_decision = 0 THEN 1 ELSE 0 END), 0) as normal_count,
            COALESCE(SUM(CASE WHEN final_decision = 1 THEN 1 ELSE 0 END), 0) as suspicious_count,
            COALESCE(SUM(CASE WHEN final_decision = 2 THEN 1 ELSE 0 END), 0) as phishing_count,
            COALESCE(SUM(CASE WHEN manual_review = 1 THEN 1 ELSE 0 END), 0) as manual_count,
            COALESCE(SUM(CASE WHEN is_block = 1 THEN 1 ELSE 0 END), 0) as block_count,
            COALESCE(SUM(CASE WHEN is_alert = 1 THEN 1 ELSE 0 END), 0) as alert_count
        FROM email_data
        WHERE created_at >= %s AND created_at <= %s
        """
        cursor.execute(summary_query, (start_time, end_time))
        summary = db_helper.fetchone_as_dict(cursor)
        
        if not summary:
            summary = {
                'total_count': 0, 'normal_count': 0, 'phishing_count': 0,
                'suspicious_count': 0, 'manual_count': 0
            }
        
        # logger.info(f"Summary: {summary}")
        
        # ============ 2. è·å–é‚®ä»¶å¤„ç†è¶‹åŠ¿æ•°æ® (å¹¶è¿›è¡Œå¡«å……) ============
        trend_query = """
        SELECT 
            UNIX_TIMESTAMP(created_at) DIV %s as time_bucket,
            COUNT(*) as total,
            COALESCE(SUM(CASE WHEN final_decision = 0 THEN 1 ELSE 0 END), 0) as normal,
            COALESCE(SUM(CASE WHEN final_decision = 1 THEN 1 ELSE 0 END), 0) as suspicious,
            COALESCE(SUM(CASE WHEN final_decision = 2 THEN 1 ELSE 0 END), 0) as phishing,
            COALESCE(SUM(CASE WHEN manual_review = 1 THEN 1 ELSE 0 END), 0) as manual
        FROM email_data
        WHERE created_at >= %s AND created_at <= %s
        GROUP BY time_bucket
        ORDER BY time_bucket ASC
        """
        cursor.execute(trend_query, (bucket_size, start_time, end_time))
        trend_raw = db_helper.fetchall_as_dict(cursor)
        
        # ä½¿ç”¨å¡«å……å‡½æ•°
        trend_data = _fill_missing_time_buckets(
            trend_raw, start_time, end_time, bucket_size, time_format, 
            ['total', 'normal', 'phishing', 'suspicious', 'manual']
        )
        
        # ============ 3. è·å–æ‹¦æˆªä¸å‘Šè­¦è¶‹åŠ¿ (å¹¶è¿›è¡Œå¡«å……) ============
        action_query = """
        SELECT 
            UNIX_TIMESTAMP(created_at) DIV %s as time_bucket,
            COALESCE(SUM(CASE WHEN is_block = 1 THEN 1 ELSE 0 END), 0) as block_count,
            COALESCE(SUM(CASE WHEN is_alert = 1 THEN 1 ELSE 0 END), 0) as alert_count
        FROM email_data
        WHERE created_at >= %s AND created_at <= %s
        GROUP BY time_bucket
        ORDER BY time_bucket ASC
        """
        cursor.execute(action_query, (bucket_size, start_time, end_time))
        action_raw = db_helper.fetchall_as_dict(cursor)
        
        # è°ƒæ•´é”®åä»¥åŒ¹é…å‰ç«¯
        for row in action_raw:
            row['block'] = row.pop('block_count')
            row['alert'] = row.pop('alert_count')

        # ä½¿ç”¨å¡«å……å‡½æ•°
        action_trend_data = _fill_missing_time_buckets(
            action_raw, start_time, end_time, bucket_size, time_format, 
            ['block', 'alert']
        )
        
        # ============ 4. è·å–æœ€è¿‘æ£€æµ‹è®°å½• (åŒ…å« AI åˆ†æå†…å®¹) ============
        records_query = """
        SELECT 
            email_id, sender,recipient,subject, created_at, final_decision,
            is_block, is_alert, manual_review, ai_reason 
        FROM email_data
        WHERE created_at >= %s AND created_at <= %s
        ORDER BY created_at DESC
        LIMIT 20
        """
        cursor.execute(records_query, (start_time, end_time))
        records_raw = db_helper.fetchall_as_dict(cursor)
        
        records = []
        result_map = {0: 'æ­£å¸¸é‚®ä»¶', 1: 'å¯ç–‘é‚®ä»¶', 2: 'é’“é±¼é‚®ä»¶'}
        
        for row in records_raw:
            # ç¡®å®šå¤„ç†çŠ¶æ€
            if row.get('is_block'):
                status = 'å·²æ‹¦æˆª'
            elif row.get('is_alert'):
                status = 'å·²å‘Šè­¦'
            elif row.get('manual_review'):
                status = 'å¾…äººå·¥ç¡®è®¤'
            else:
                status = 'å·²æ”¾è¡Œ'
            
            # å¤„ç†æ—¶é—´æ ¼å¼ - ç»Ÿä¸€ä½¿ç”¨ created_at
            created_at = row.get('created_at')
            if created_at:
                if isinstance(created_at, str):
                    time_str = created_at
                elif isinstance(created_at, datetime):
                    time_str = created_at.strftime('%Y-%m-%d %H:%M:%S')
                else:
                    time_str = str(created_at)
            else:
                time_str = 'æœªçŸ¥'
            
            records.append({
                'id': row.get('email_id', ''),
                'sender': row.get('sender') or 'æœªçŸ¥',
                'recipient': row.get('recipient') or 'æœªçŸ¥',
                'subject': row.get('subject') or 'æ— ä¸»é¢˜',
                'time': time_str,
                'result': result_map.get(row.get('final_decision'), 'æœªçŸ¥'),
                'status': status,
                # ğŸ’¥ å…³é”®ä¿®æ­£ï¼šç»Ÿä¸€å­—æ®µåä»¥åŒ¹é…å‰ç«¯ Dashboard.vue
                'ai_reason': row.get('ai_reason') or 'æ— ' 
            })
        
        # ============ 5. è¿”å›æ•°æ® (æ ¸å¿ƒä¿®æ”¹ï¼šæ·»åŠ codeå­—æ®µï¼Œé€‚é…å‰ç«¯æ‹¦æˆªå™¨) ============
        return jsonify({
            'code': 200,  # æ–°å¢ï¼šå‰ç«¯æ‹¦æˆªå™¨éœ€è¦çš„codeå­—æ®µ
            'success': True,
            'message': 'ä»ªè¡¨ç›˜æ•°æ®æŸ¥è¯¢æˆåŠŸ',  # æ–°å¢ï¼šå‰ç«¯å¯æ•è·çš„æç¤ºä¿¡æ¯
            'data': {
                'summary': {
                    'totalCount': int(summary.get('total_count', 0)),
                    'normalCount': int(summary.get('normal_count', 0)),
                    'phishingCount': int(summary.get('phishing_count', 0)),
                    'suspiciousCount': int(summary.get('suspicious_count', 0)),
                    'manualCount': int(summary.get('manual_count', 0)),
                },
                'trendData': trend_data,
                'actionTrendData': action_trend_data,
                'records': records
            }
        })
        
    except Exception as e:
        logger.error(f"Dashboard API error: {str(e)}", exc_info=True)
        return jsonify({
            'code': 500,  # æ–°å¢ï¼šé”™è¯¯ç 
            'success': False,
            'message': f'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼š{str(e)}',  # æ–°å¢ï¼šé”™è¯¯ä¿¡æ¯
            'error': str(e)
        }), 500
    
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()